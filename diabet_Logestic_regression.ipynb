{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0b0daf19-5bae-4192-9835-04378dc6ad26",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "from typing import Optional\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from torch.utils.data import TensorDataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "44df9dde-8322-4e97-80d7-7cc445b9451b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('2.2.3', '2.5.1', True)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.__version__, torch.__version__, torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "93904ebb-1992-4023-93ff-30134ddc0360",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiabetClassification:\n",
    "    \n",
    "    def __init__(self, *, dataset_url: str,train_batch_size: int, val_batch_size: int, test_batch_size: int, epoch_num: int):\n",
    "        self.dataset_url: str= dataset_url\n",
    "        self.features: Optional[pd.Series]= None\n",
    "        self.label: Optional[pd.Series]= None\n",
    "        self.num_features: int= 0\n",
    "        self.epoch_num: int= epoch_num\n",
    "        self.X_train: torch.Tensor= None\n",
    "        self.y_train: torch.Tensor= None\n",
    "        self.X_val: torch.Tensor= None\n",
    "        self.y_valid: torch.Tensor= None\n",
    "        self.X_test: torch.Tensor= None\n",
    "        self.y_test: torch.Tensor= None\n",
    "        self.train_batch_size: int= train_batch_size\n",
    "        self.val_batch_size: int= val_batch_size\n",
    "        self.test_batch_size: int= test_batch_size\n",
    "        self.train_set: torch.Tensor= None\n",
    "        self.val_set: torch.Tensor= None\n",
    "        self.test_set: torch.Tensor= None\n",
    "        self.train_loader: torch.Tensor= None\n",
    "        self.val_loader: torch.Tensor= None\n",
    "        self.test_loader: torch.Tensor= None\n",
    "        self.model: nn.Module= None\n",
    "        self.loss_fn: Optional[Callable]= None\n",
    "        self.optimizer: Optional[OptimizerType]= None\n",
    "        \n",
    "    \n",
    "    def _load_dataset(self) -> None:\n",
    "        \"\"\"  \n",
    "        Loads the dataset from a CSV file and splits it into features and labels.  \n",
    "        The features exclude the 'Outcome' column, and the label is reshaped.  \n",
    "        \"\"\"  \n",
    "        df= pd.read_csv(self.dataset_url)\n",
    "        self.num_features= df.shape[1] - 1 # number of features\n",
    "        self.features= df.drop('Outcome', axis=1).values # Features\n",
    "        self.label= df['Outcome'].values.reshape(-1, 1) # Label\n",
    "\n",
    "    \n",
    "    def _split_data(self)-> None:\n",
    "        \"\"\"  \n",
    "        Splits the dataset into training, validation, and test sets.  \n",
    "        The training set receives 72% of the data, validation set 8%, and test set 20%.  \n",
    "        \"\"\"  \n",
    "        X_train, X_test, y_train, y_test= train_test_split(\n",
    "            self.features,\n",
    "            self.label,\n",
    "            test_size=0.2,\n",
    "            random_state= 42)\n",
    "\n",
    "        X_train, X_val, y_train, y_val= train_test_split(\n",
    "            X_train,\n",
    "            y_train,\n",
    "            test_size= 0.1,\n",
    "            random_state= 42)\n",
    "\n",
    "        self.X_train= X_train\n",
    "        self.y_train= y_train\n",
    "        self.X_val= X_val\n",
    "        self.y_val= y_val\n",
    "        self.X_test= X_test\n",
    "        self.y_test= y_test\n",
    "\n",
    "    \n",
    "    def _normalization(self)-> None:\n",
    "        \"\"\"  \n",
    "        Normalizes the training, validation, and test datasets using StandardScaler.  \n",
    "        Transforms the features to have a mean of 0 and a standard deviation of 1.  \n",
    "        \"\"\"  \n",
    "        x_scaler= StandardScaler()\n",
    "        \n",
    "        self.X_train= x_scaler.fit_transform(self.X_train)\n",
    "        self.X_val= x_scaler.transform(self.X_val)\n",
    "        self.X_test= x_scaler.transform(self.X_test)\n",
    "        \n",
    "    \n",
    "    def _to_tensor(self)-> None:\n",
    "        \"\"\"  \n",
    "        Converts training, validation, and test datasets to PyTorch tensors   \n",
    "        with float32 data type.  \n",
    "        \"\"\"         \n",
    "        self.X_train= torch.tensor(self.X_train, dtype= torch.float32)\n",
    "        self.y_train= torch.tensor(self.y_train, dtype= torch.float32)\n",
    "        self.X_val= torch.tensor(self.X_val, dtype= torch.float32)\n",
    "        self.y_val= torch.tensor(self.y_val, dtype= torch.float32)\n",
    "        self.X_test= torch.tensor(self.X_test, dtype= torch.float32)\n",
    "        self.y_test= torch.tensor(self.y_test, dtype= torch.float32)\n",
    "\n",
    "    \n",
    "    def _create_dataloader(self):\n",
    "        \"\"\"  \n",
    "        Creates DataLoader objects for training, validation, and test datasets.  \n",
    "        Initializes:  \n",
    "            - train_loader: DataLoader for training data (shuffled).  \n",
    "            - val_loader: DataLoader for validation data.  \n",
    "            - test_loader: DataLoader for test data.  \n",
    "        \"\"\"         \n",
    "        self.train_set= TensorDataset(self.X_train, self.y_train)\n",
    "        self.val_set= TensorDataset(self.X_val, self.y_val)\n",
    "        self.test_set= TensorDataset(self.X_test, self.y_test)\n",
    "\n",
    "        self.train_loader= DataLoader(self.train_set, batch_size= self.train_batch_size, shuffle= True)\n",
    "        self.val_loader= DataLoader(self.val_set, batch_size= self.val_batch_size)\n",
    "        self.test_loader= DataLoader(self.test_set, batch_size= self.test_batch_size)\n",
    "\n",
    "\n",
    "    def _build_model(self):   \n",
    "        \"\"\"  \n",
    "        Builds a neural network model with a linear layer and sigmoid activation.    \n",
    "        This model is used for binary classification, taking `num_features` as input  \n",
    "        and outputting a probability value between 0 and 1.  \n",
    "    \n",
    "        Attributes:  \n",
    "            model (nn.Sequential): The constructed neural network model.  \n",
    "        \"\"\"         \n",
    "        self.model= nn.Sequential(\n",
    "           nn.Linear(in_features= self.num_features, out_features=1),\n",
    "           nn.Sigmoid())\n",
    "\n",
    "    def _compute_loss(self):\n",
    "        self.loss_fn = nn.BCELoss()\n",
    "\n",
    "    \n",
    "    def _setup_optimizer(self):\n",
    "        self.optimizer = torch.optim.SGD(self.model.parameters(), lr=0.05, momentum= 0.9)\n",
    "\n",
    "\n",
    "    def _train(self):\n",
    "        for epoch in range(self.epoch_num):\n",
    "            sum_train_loss, sum_train_acc = 0, 0\n",
    "            \n",
    "            for x_batch, y_batch in self.train_loader:\n",
    "                y_hat= self.model(x_batch) # model\n",
    "                loss= self.loss_fn(y_hat, y_batch) # loss\n",
    "                loss.backward() # gradient\n",
    "\n",
    "                self.optimizer.step() # update\n",
    "                self.optimizer.zero_grad()\n",
    "                \n",
    "                sum_train_loss += loss * len(x_batch)\n",
    "                sum_train_acc +=torch.sum(y_hat.round() == y_batch).item()\n",
    "                \n",
    "            mean_train_loss= sum_train_loss / len(self.train_set)\n",
    "            mean_train_acc= sum_train_acc / len(self.train_set)\n",
    "\n",
    "            with torch.no_grad():           \n",
    "                sum_val_loss, sum_val_acc= 0, 0\n",
    "                \n",
    "                for x_batch, y_batch in self.val_loader:\n",
    "                    y_hat= self.model(x_batch)\n",
    "                    loss= self.loss_fn(y_hat, y_batch)\n",
    "\n",
    "                    sum_val_loss += loss * len(x_batch)\n",
    "                    sum_val_acc += torch.sum(y_hat.round() == y_batch).item()\n",
    "                    \n",
    "                mean_val_loss= sum_val_loss / len(self.val_set)\n",
    "                mean_val_acc= sum_val_acc / len(self.val_set)\n",
    "            \n",
    "            print(\n",
    "                f'[epoch: {epoch}] ,',\n",
    "                f'Train_loss: {mean_train_loss :.2f}, ',\n",
    "                f'Train_acc: {mean_train_acc :.2f}, ',\n",
    "                f'val_loss: {mean_val_loss :.2f}, ',\n",
    "                f'val_acc: {mean_val_acc :.2f}, '\n",
    "                )\n",
    "            \n",
    "    def _evaluate(self):\n",
    "        with torch.no_grad():\n",
    "            sum_test_acc= 0\n",
    "            \n",
    "            for x_batch, y_batch in self.test_loader:\n",
    "                y_hat= self.model(x_batch)\n",
    "                sum_test_acc += torch.sum(y_hat.round() == y_batch).item()\n",
    "\n",
    "            mean_test_acc= sum_test_acc / len(self.test_set)\n",
    "            print(f'\\nTest accuracy: {mean_test_acc :.2f}')                \n",
    "                \n",
    "    \n",
    "    def __call__(self):\n",
    "        self._load_dataset()\n",
    "        self._split_data()\n",
    "        self._normalization()\n",
    "        self._to_tensor()\n",
    "        self._create_dataloader()\n",
    "        self._build_model()\n",
    "        self._compute_loss()\n",
    "        self._setup_optimizer()\n",
    "        self._train()\n",
    "        self._evaluate()\n",
    "\n",
    "    def __str__(self):\n",
    "        parameters= [\n",
    "            f'model weights: {self.model[0].weight}',\n",
    "            f'model bias: {self.model[0].bias}',\n",
    "        ]\n",
    "        return ','.join(parameters)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5dbb6a24-795a-4ddf-9541-278c97685b48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch: 0] , Train_loss: 0.63,  Train_acc: 0.70,  val_loss: 0.60,  val_acc: 0.66, \n",
      "[epoch: 1] , Train_loss: 0.55,  Train_acc: 0.75,  val_loss: 0.48,  val_acc: 0.79, \n",
      "[epoch: 2] , Train_loss: 0.50,  Train_acc: 0.76,  val_loss: 0.45,  val_acc: 0.79, \n",
      "[epoch: 3] , Train_loss: 0.49,  Train_acc: 0.76,  val_loss: 0.44,  val_acc: 0.81, \n",
      "[epoch: 4] , Train_loss: 0.48,  Train_acc: 0.77,  val_loss: 0.45,  val_acc: 0.82, \n",
      "[epoch: 5] , Train_loss: 0.47,  Train_acc: 0.76,  val_loss: 0.45,  val_acc: 0.81, \n",
      "[epoch: 6] , Train_loss: 0.47,  Train_acc: 0.76,  val_loss: 0.45,  val_acc: 0.82, \n",
      "[epoch: 7] , Train_loss: 0.47,  Train_acc: 0.77,  val_loss: 0.46,  val_acc: 0.82, \n",
      "[epoch: 8] , Train_loss: 0.47,  Train_acc: 0.77,  val_loss: 0.46,  val_acc: 0.82, \n",
      "[epoch: 9] , Train_loss: 0.47,  Train_acc: 0.77,  val_loss: 0.46,  val_acc: 0.82, \n",
      "\n",
      "Test accuracy: 0.74\n"
     ]
    }
   ],
   "source": [
    "classifier_obj= DiabetClassification(\n",
    "    dataset_url= 'diabetes.csv',\n",
    "    train_batch_size= 100,\n",
    "    val_batch_size=50, \n",
    "    test_batch_size=50,\n",
    "    epoch_num= 10)        \n",
    "\n",
    "classifier_obj()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1c8d617a-701c-4759-877d-4f664f61a13a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([552, 8]), torch.Size([62, 8]), torch.Size([154, 8]))"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier_obj.X_train.shape, classifier_obj.X_val.shape, classifier_obj.X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "43b28845-c399-45a7-877d-71b99abd7743",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([50, 8]), torch.Size([50, 1]), 6)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_batch, y_batch = next(iter(classifier_obj.val_loader))\n",
    "x_batch.shape, y_batch.shape, len(classifier_obj.train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "bd289c0e-59a5-4d04-9275-0795ce0fc27b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier_obj.epoch_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "463ce551-e26d-4e96-805f-0237ef3d3674",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
